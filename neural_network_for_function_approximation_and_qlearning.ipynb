{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_network_for_function_approximation_and_qlearning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMS/PVHw7mKcDM/QjOjXjQ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Iramuk-ganh/rl/blob/main/neural_network_for_function_approximation_and_qlearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giA8XyBWU6yP",
        "outputId": "311b9cfb-fc4a-4dff-b737-eddf9c522ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 155229 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "    %tensorflow_version 1.x\n",
        "    \n",
        "    if not os.path.exists('.setup_complete'):\n",
        "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week4_approx/submit.py\n",
        "\n",
        "        !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "s-p9Wh8gU9xr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v0\").env"
      ],
      "metadata": {
        "id": "NB5WDQ4YU_1b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "no_of_actions = env.action_space.n\n",
        "obs_state_dim = env.observation_space.shape\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "HHSnEhhYVHxR",
        "outputId": "73c690de-6194-46f9-d67a-d03d61a26d45"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f585e339390>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Building neural network for action-value function approximation"
      ],
      "metadata": {
        "id": "iJMUiBw2VxhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras.layers as L"
      ],
      "metadata": {
        "id": "jy3aO0d3VZqj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "keras.backend.set_session(sess)"
      ],
      "metadata": {
        "id": "q1xwbMLBWGai"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neural_network = keras.models.Sequential()\n",
        "neural_network.add(L.InputLayer(obs_state_dim))\n",
        "neural_network.add(L.Dense(200, activation='relu'))#relu is a non-saturating non-linearity\n",
        "neural_network.add(L.Dense(100, activation='relu'))\n",
        "neural_network.add(L.Dense(no_of_actions, activation = 'linear'))"
      ],
      "metadata": {
        "id": "8e-xpuH7WVbb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "😌np.random.rand() when no arguments passed returns a random decimal number between 0 and 1"
      ],
      "metadata": {
        "id": "ktljd-qLY4KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.rand()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyIjekXhYXZ9",
        "outputId": "195ec26a-d411-473f-f3c6-5b9db9976624"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.28534617259920536"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_actions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjvV8jhqaM4_",
        "outputId": "2d151cef-7dbf-49c3-fbe2-040bf2232c29"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.choice(no_of_actions, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVyo7BRTaE4E",
        "outputId": "b7f5d6a7-f733-46da-e990-208145f48eb1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_action(state, epsilon=0):\n",
        "    \"\"\"\n",
        "    sample actions with epsilon-greedy policy\n",
        "    recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
        "    \"\"\"\n",
        "    \n",
        "    q_values = neural_network.predict(state[None])[0]#neural_network outputs action_value functions for a particular state and all actions\n",
        "    #q_values is an array with q_value corresponding to each action i.e no. of elements in q_values = no_of_actions\n",
        "    \n",
        "    prob = np.random.rand()\n",
        "\n",
        "    if prob < epsilon:\n",
        "      action = np.random.choice(no_of_actions, 1)[0]\n",
        "\n",
        "    else:\n",
        "      action = np.argmax(q_values)\n",
        "\n",
        "    return action"
      ],
      "metadata": {
        "id": "NBT0t_mrXa2F"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.choice([0, 1], p=(0.1, 1-0.1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "757UyKNdbbYW",
        "outputId": "6c52fa4f-96be-4b6f-c2fa-d006f3fb6ff7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_action(state, epsilon=0):\n",
        "    \"\"\"\n",
        "    sample actions with epsilon-greedy policy\n",
        "    recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
        "    \"\"\"\n",
        "    \n",
        "    q_values = neural_network.predict(state[None])[0]\n",
        "    \n",
        "    choice = np.random.choice([0, 1], p=(epsilon, 1-epsilon))\n",
        "\n",
        "    if choice:#explore or take random actions\n",
        "      action = np.random.choice(no_of_actions, 1)[0]\n",
        "\n",
        "    else:#exploit or take the best action\n",
        "      action = np.argmax(q_values)\n",
        "\n",
        "    return action"
      ],
      "metadata": {
        "id": "uFSzNQhkbDfF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert neural_network.output_shape == (None, no_of_actions), \"please make sure your model maps state s -> [Q(s,a0), ..., Q(s, a_last)]\"\n",
        "assert neural_network.layers[-1].activation == keras.activations.linear, \"please make sure you predict q-values without nonlinearity\"\n",
        "\n",
        "# test epsilon-greedy exploration\n",
        "s = env.reset()\n",
        "assert np.shape(get_action(s)) == (), \"please return just one action (integer)\"\n",
        "for eps in [0., 0.1, 0.5, 1.0]:\n",
        "    state_frequencies = np.bincount([get_action(s, epsilon=eps) for i in range(10000)], minlength=no_of_actions)\n",
        "    best_action = state_frequencies.argmax()\n",
        "    assert abs(state_frequencies[best_action] - 10000 * (1 - eps + eps / no_of_actions)) < 200\n",
        "    for other_action in range(no_of_actions):\n",
        "        if other_action != best_action:\n",
        "            assert abs(state_frequencies[other_action] - 10000 * (eps / no_of_actions)) < 200\n",
        "    print('e=%.1f tests passed'%eps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbUBGUuhcFLe",
        "outputId": "59abf4a4-bfbf-4321-b4bc-de65373b1f8b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e=0.0 tests passed\n",
            "e=0.1 tests passed\n",
            "e=0.5 tests passed\n",
            "e=1.0 tests passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q-learning via gradient descent"
      ],
      "metadata": {
        "id": "CpoZEfmpc3Ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create placeholders for the <s, a, r, s'> tuple and a special indicator for game end (is_done = True)\n",
        "states_ph = keras.backend.placeholder(dtype='float32', shape=(None,) + obs_state_dim)\n",
        "actions_ph = keras.backend.placeholder(dtype='int32', shape=[None])\n",
        "rewards_ph = keras.backend.placeholder(dtype='float32', shape=[None])\n",
        "next_states_ph = keras.backend.placeholder(dtype='float32', shape=(None,) + obs_state_dim)\n",
        "is_done_ph = keras.backend.placeholder(dtype='bool', shape=[None])"
      ],
      "metadata": {
        "id": "pkPWewFNcJvg"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "😌tf.one_hot()\n",
        "\n",
        "indices = [0, 1, 2]\n",
        "depth = 3\n",
        "tf.one_hot(indices, depth) \n",
        " output: [3 x 3]\n",
        "\n",
        " [[1., 0., 0.],\n",
        "\n",
        "  [0., 1., 0.],\n",
        "\n",
        "  [0., 0., 1.]]"
      ],
      "metadata": {
        "id": "ahyJXoEGfdvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "😌tf.reduce_sum()\n",
        "\n",
        "x = tf.constant([[1, 1, 1], [1, 1, 1]])\n",
        "\n",
        "tf.reduce_sum(x)  # 6\n",
        "\n",
        "tf.reduce_sum(x, 0)  # [2, 2, 2]\n",
        "\n",
        "tf.reduce_sum(x, 1)  # [3, 3]"
      ],
      "metadata": {
        "id": "vVF3w1sMf9z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get q-values for all actions in current states\n",
        "predicted_qvalues = neural_network(states_ph)\n",
        "\n",
        "#select q-values for chosen actions\n",
        "predicted_qvalues_for_actions = tf.reduce_sum(predicted_qvalues * tf.one_hot(actions_ph, no_of_actions), axis = 1)"
      ],
      "metadata": {
        "id": "h4Sk1Yecd40a"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.99\n",
        "\n",
        "# compute q-values for all actions in next states\n",
        "predicted_next_qvalues = neural_network(next_states_ph)\n",
        "\n",
        "# compute V*(next_states) using predicted next q-values\n",
        "next_state_values = tf.reduce_max(predicted_next_qvalues, axis = 1)#max_qvalue for the next state over all possible actions\n",
        "\n",
        "# compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
        "target_qvalues_for_actions = rewards_ph + gamma * next_state_values\n",
        "\n",
        "# at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
        "target_qvalues_for_actions = tf.where(is_done_ph, rewards_ph, target_qvalues_for_actions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYrvGOpUgmTq",
        "outputId": "0a63a083-1376-4b93-b83f-438c76b99c2c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-43-12a95bf1d2cb>:13: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mean squared error loss to minimize\n",
        "loss = (predicted_qvalues_for_actions - tf.stop_gradient(target_qvalues_for_actions)) ** 2\n",
        "loss = tf.reduce_mean(loss)\n",
        "\n",
        "# training function that resembles agent.update(state, action, reward, next_state) from tabular agent\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
      ],
      "metadata": {
        "id": "PCuwDP9Wigc2"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert tf.gradients(loss, [predicted_qvalues_for_actions])[0] is not None, \"make sure you update q-values for chosen actions and not just all actions\"\n",
        "assert tf.gradients(loss, [predicted_next_qvalues])[0] is None, \"make sure you don't propagate gradient w.r.t. Q_(s',a')\"\n",
        "assert predicted_next_qvalues.shape.ndims == 2, \"make sure you predicted q-values for all actions in next state\"\n",
        "assert next_state_values.shape.ndims == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
        "assert target_qvalues_for_actions.shape.ndims == 1, \"there's something wrong with target q-values, they must be a vector\""
      ],
      "metadata": {
        "id": "kj6Woj7ei0De"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Playing the game"
      ],
      "metadata": {
        "id": "u-GXNPA7i5hO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "❓sess.run()"
      ],
      "metadata": {
        "id": "07d3ZLcaoihB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sess.run(tf.global_variables_initializer())"
      ],
      "metadata": {
        "id": "V7tc3B5gi3Ki"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_session(env, t_max = 1000, epsilon = 0, train = False):\n",
        "  total_reward = 0\n",
        "  s = env.reset()\n",
        "\n",
        "  for t in range(t_max):\n",
        "    a = get_action(s, epsilon=epsilon)\n",
        "    next_s, r, done, _ = env.step(a)\n",
        "\n",
        "    if train:\n",
        "      sess.run(train_step,\n",
        "               {\n",
        "                states_ph: [s], \n",
        "                actions_ph: [a],\n",
        "                rewards_ph: [r],\n",
        "                next_states_ph: [next_s],\n",
        "                is_done_ph: [done]\n",
        "               })\n",
        "      \n",
        "      total_reward += r\n",
        "      s = next_s\n",
        "      if done:\n",
        "        break\n",
        "  return total_reward"
      ],
      "metadata": {
        "id": "KVafcEJ7i-yk"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.5"
      ],
      "metadata": {
        "id": "0AbANK6DnPr2"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "  session_rewards = [generate_session(env, epsilon=epsilon, train=True) for _ in range(100)]\n",
        "  print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
        "    \n",
        "  epsilon *= 0.99\n",
        "  assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
        "  \n",
        "  if np.mean(session_rewards) > 300:\n",
        "      print(\"You Win!\")\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXZIDPJQnS45",
        "outputId": "b822bb1c-cfa9-4b5f-de75-89bf6a679293"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch #0\tmean reward = 16.080\tepsilon = 0.500\n",
            "epoch #1\tmean reward = 14.620\tepsilon = 0.495\n",
            "epoch #2\tmean reward = 17.770\tepsilon = 0.490\n",
            "epoch #3\tmean reward = 14.840\tepsilon = 0.485\n",
            "epoch #4\tmean reward = 15.780\tepsilon = 0.480\n",
            "epoch #5\tmean reward = 15.690\tepsilon = 0.475\n",
            "epoch #6\tmean reward = 15.670\tepsilon = 0.471\n",
            "epoch #7\tmean reward = 34.870\tepsilon = 0.466\n",
            "epoch #8\tmean reward = 32.070\tepsilon = 0.461\n",
            "epoch #9\tmean reward = 36.040\tepsilon = 0.457\n",
            "epoch #10\tmean reward = 43.300\tepsilon = 0.452\n",
            "epoch #11\tmean reward = 58.180\tepsilon = 0.448\n",
            "epoch #12\tmean reward = 66.470\tepsilon = 0.443\n",
            "epoch #13\tmean reward = 107.610\tepsilon = 0.439\n",
            "epoch #14\tmean reward = 124.850\tepsilon = 0.434\n",
            "epoch #15\tmean reward = 138.640\tepsilon = 0.430\n",
            "epoch #16\tmean reward = 157.890\tepsilon = 0.426\n",
            "epoch #17\tmean reward = 164.270\tepsilon = 0.421\n",
            "epoch #18\tmean reward = 188.410\tepsilon = 0.417\n",
            "epoch #19\tmean reward = 205.360\tepsilon = 0.413\n",
            "epoch #20\tmean reward = 197.640\tepsilon = 0.409\n",
            "epoch #21\tmean reward = 250.380\tepsilon = 0.405\n",
            "epoch #22\tmean reward = 217.070\tepsilon = 0.401\n",
            "epoch #23\tmean reward = 329.630\tepsilon = 0.397\n",
            "You Win!\n"
          ]
        }
      ]
    }
  ]
}